{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qwIIB80e3jwHlDKGozE6DuYz9HFFDGaW",
      "authorship_tag": "ABX9TyO+roEOz9P8gTCrgbUOoHvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adyasha95/PredictionTendinopathy/blob/main/predictioninjury.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "CgKNqRjU4xV-",
        "outputId": "b0a3f337-e1c8-4049-c2ea-d9096b4bf7a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-480e88af-f5e3-4ab7-9793-5932ed97d6b4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-480e88af-f5e3-4ab7-9793-5932ed97d6b4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Final Dastaset Oct25.xlsx to Final Dastaset Oct25 (1).xlsx\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")  # authorize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn xgboost lightgbm catboost openpyxl"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jH9PIfL0B1Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Tendinopathy — NeuroMiner-style Nested CV (Leakage-Safe)\n",
        "# =========================\n",
        "import os, json, time, warnings, numpy as np, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- sklearn / imblearn\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, recall_score, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "\n",
        "# --- boosters (optional)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "\n",
        "# =========================\n",
        "# Config (reduced folds for Oct 20 brief)\n",
        "# =========================\n",
        "@dataclass\n",
        "class Config:\n",
        "    DATA_PATH: str = \"/content/Final Dastaset Oct25.xlsx\"     # <-- set for Colab\n",
        "    TARGET_NAME_PREFERRED: str = \"Tendinopathy Injury\"\n",
        "    TARGET_EXCEL_LETTER: str = \"BI\"\n",
        "    ORDINAL_RANGES: Tuple[str, ...] = (\"AH:AO\", \"AQ:BG\")\n",
        "    RANDOM_STATE: int = 42\n",
        "    OUTER_FOLDS: int = 2     # reduced for quick report; set 5 for final\n",
        "    INNER_FOLDS: int = 3     # reduced for quick report; set 5 for final\n",
        "    N_TRIALS: int = 5        # modest tuning for speed; increase later\n",
        "    RESULTS_DIR: str = \"./results_tendinopathy_nm\"\n",
        "    NUISANCE_COVARS: Tuple[str, ...] = (\"Age\",)  # example covariate(s) to regress out if present\n",
        "    MISSING_THRESHOLD: float = 0.8  # Drop columns with more than this % missing\n",
        "\n",
        "CFG = Config()\n",
        "os.makedirs(CFG.RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Utilities\n",
        "# =========================\n",
        "def excel_letter_to_index(letter: str) -> int:\n",
        "    letter = letter.strip().upper()\n",
        "    n = 0\n",
        "    for ch in letter:\n",
        "        n = n * 26 + (ord(ch) - 64)\n",
        "    return n - 1\n",
        "\n",
        "def expand_excel_range_to_names(df: pd.DataFrame, excel_range: str) -> List[str]:\n",
        "    a, b = [s.strip().upper() for s in excel_range.split(\":\")]\n",
        "    i1, i2 = excel_letter_to_index(a), excel_letter_to_index(b)\n",
        "    lo, hi = min(i1, i2), max(i1, i2)\n",
        "    cols = df.columns.tolist()\n",
        "    hi = min(hi, len(cols) - 1); lo = max(lo, 0)\n",
        "    return cols[lo:hi+1]\n",
        "\n",
        "def binarize_yes_no(series: pd.Series) -> np.ndarray:\n",
        "    # Handle potential NaNs or other values before mapping\n",
        "    return series.astype(str).str.strip().str.lower().map({\"yes\":1,\"no\":0}).fillna(-1).astype(int).values # Map NaNs to -1 or another indicator\n",
        "\n",
        "def specificity_score(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr=0.5) -> Dict[str,float]:\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    sens = recall_score(y_true, y_pred)\n",
        "    spec = specificity_score(y_true, y_pred)\n",
        "    bac  = 0.5*(sens+spec)\n",
        "    roc  = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
        "    pr   = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
        "    return {\"Sensitivity\": sens, \"Specificity\": spec, \"BAC\": bac, \"ROC_AUC\": roc, \"PR_AUC\": pr}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Covariate Residualizer (NM-style correction inside folds)\n",
        "# Regresses numeric features on nuisance covariates using only training data,\n",
        "# then replaces features by residuals on both train and test in a fold.\n",
        "# =========================\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "class ResidualizeCovariates(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Residualizes numeric feature columns with respect to given covariates.\n",
        "\n",
        "    NaN-safe:\n",
        "      - During FIT: for each feature col, fit LinearRegression on rows where\n",
        "        both covariates and that feature are finite.\n",
        "      - Store covariate medians from the TRAINING rows to impute covariates\n",
        "        at TRANSFORM time for prediction only.\n",
        "      - During TRANSFORM: subtract prediction ONLY where the original feature\n",
        "        is finite; leave NaNs as NaN so downstream imputers can handle them.\n",
        "    \"\"\"\n",
        "    def __init__(self, covariate_cols):\n",
        "        self.covariate_cols = covariate_cols\n",
        "        self.models_ = {}            # col -> LinearRegression\n",
        "        self.cov_medians_ = None     # medians for covariate imputation at predict time\n",
        "        self.feature_cols_ = None\n",
        "        self.covars_present_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if not hasattr(X, \"columns\"):\n",
        "            # If X isn't a DataFrame, skip residualization\n",
        "            self.feature_cols_ = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "            self.covars_present_ = []\n",
        "            return self\n",
        "\n",
        "        self.feature_cols_ = list(X.columns)\n",
        "        self.covars_present_ = [c for c in (self.covariate_cols or []) if c in X.columns]\n",
        "        if len(self.covars_present_) == 0:\n",
        "            return self\n",
        "\n",
        "        # covariate matrix\n",
        "        C = X[self.covars_present_].to_numpy()\n",
        "        # compute covariate medians on rows that are fully finite in C\n",
        "        C_finite_mask = np.isfinite(C).all(axis=1)\n",
        "        if C_finite_mask.any():\n",
        "            self.cov_medians_ = np.nanmedian(C[C_finite_mask], axis=0)\n",
        "        else:\n",
        "            # fallback: column-wise median ignoring NaNs\n",
        "            self.cov_medians_ = np.nanmedian(C, axis=0)\n",
        "\n",
        "        # Fit a model per numeric feature where possible\n",
        "        for col in self.feature_cols_:\n",
        "            if col in self.covars_present_:\n",
        "                continue\n",
        "            xi = X[col].to_numpy()\n",
        "            # only numeric columns\n",
        "            if not np.issubdtype(np.asarray(xi).dtype, np.number):\n",
        "                continue\n",
        "\n",
        "            # mask: rows with finite covariates AND finite xi\n",
        "            mask = np.isfinite(xi) & np.isfinite(C).all(axis=1)\n",
        "            if mask.sum() < 5:\n",
        "                # too few rows to fit reliably; skip this feature\n",
        "                continue\n",
        "\n",
        "            lr = LinearRegression()\n",
        "            lr.fit(C[mask], xi[mask])\n",
        "            self.models_[col] = lr\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not hasattr(X, \"copy\") or len(self.models_) == 0:\n",
        "            return X\n",
        "\n",
        "        Xr = X.copy()\n",
        "        # Build covariate matrix and impute NaNs in covariates with stored medians for prediction\n",
        "        if len(self.covars_present_) > 0:\n",
        "            C = Xr[self.covars_present_].to_numpy()\n",
        "            C_imp = C.copy()\n",
        "            if self.cov_medians_ is None:\n",
        "                # if somehow unset, compute simple medians per column\n",
        "                med = np.nanmedian(C_imp, axis=0)\n",
        "            else:\n",
        "                med = self.cov_medians_\n",
        "            # fill NaNs in covariates for prediction only\n",
        "            nan_mask = ~np.isfinite(C_imp)\n",
        "            if nan_mask.any():\n",
        "                # broadcast medians to NaN positions\n",
        "                C_imp[nan_mask] = np.take(med, np.where(nan_mask)[1])\n",
        "\n",
        "            # Apply per-feature residualization\n",
        "            for col, lr in self.models_.items():\n",
        "                if col in Xr.columns:\n",
        "                    xi = Xr[col].to_numpy()\n",
        "                    pred = lr.predict(C_imp)\n",
        "                    # subtract only where xi is finite; preserve NaNs to be imputed later\n",
        "                    finite_mask = np.isfinite(xi)\n",
        "                    xi_resid = xi.copy()\n",
        "                    xi_resid[finite_mask] = xi[finite_mask] - pred[finite_mask]\n",
        "                    Xr[col] = xi_resid\n",
        "        return Xr\n",
        "# =========================\n",
        "# Load data & declare feature sets\n",
        "# =========================\n",
        "assert os.path.exists(CFG.DATA_PATH), f\"File not found: {CFG.DATA_PATH}\"\n",
        "df = pd.read_excel(CFG.DATA_PATH)\n",
        "\n",
        "# pick target by name or by Excel letter (BI)\n",
        "if CFG.TARGET_NAME_PREFERRED in df.columns:\n",
        "    target_col = CFG.TARGET_NAME_PREFERRED\n",
        "else:\n",
        "    target_col = df.columns[excel_letter_to_index(CFG.TARGET_EXCEL_LETTER)]\n",
        "\n",
        "# Handle potential NaNs in the target column before binarization\n",
        "df = df.dropna(subset=[target_col]).copy()\n",
        "\n",
        "# Drop leakage / metadata columns (injury description etc.)\n",
        "drop_cols = [\n",
        "    'Date of Injury', 'Location', 'Affected tendon', 'Leg./arm Injured',\n",
        "    'Injury severity', 'If other, please write',\n",
        "    'No. of partial games participation before injury',\n",
        "    'No. of training sessions before injury',\n",
        "    'No. of full games participation before injury', # This column has many NaNs, drop it.\n",
        "    'Code'\n",
        "]\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "# Drop columns with excessive missing values\n",
        "missing_counts = df.isnull().sum()\n",
        "cols_to_drop_missing = missing_counts[missing_counts / len(df) > CFG.MISSING_THRESHOLD].index.tolist()\n",
        "df = df.drop(columns=cols_to_drop_missing, errors='ignore')\n",
        "\n",
        "\n",
        "# Ordinals by Excel ranges (as requested)\n",
        "ordinal_cols = []\n",
        "for r in CFG.ORDINAL_RANGES:\n",
        "    ordinal_cols += [c for c in expand_excel_range_to_names(df, r) if c != target_col and c in df.columns]\n",
        "ordinal_cols = list(dict.fromkeys(ordinal_cols))\n",
        "\n",
        "y = binarize_yes_no(df[target_col])\n",
        "X = df.drop(columns=[target_col]).copy()\n",
        "\n",
        "num_cols  = [c for c in X.select_dtypes(include=[np.number]).columns if c not in ordinal_cols]\n",
        "cat_cols  = [c for c in X.select_dtypes(exclude=[np.number]).columns if c not in ordinal_cols]\n",
        "ord_num   = [c for c in ordinal_cols if c in X.columns and pd.api.types.is_numeric_dtype(X[c])]\n",
        "ord_cat   = [c for c in ordinal_cols if c in X.columns and not pd.api.types.is_numeric_dtype(X[c])]\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Build preprocessing (NOT FIT HERE) — will be cloned & fit INSIDE CV ONLY\n",
        "# Order: Residualize (raw df) -> ColumnTransformer(Impute/Encode/Scale)\n",
        "# =========================\n",
        "# =========================================================\n",
        "# 🧠 CombinedPreprocessor Fix\n",
        "# =========================================================\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "class CombinedPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Single transformer that does:\n",
        "      ResidualizeCovariates (on pandas DataFrame)\n",
        "      -> ColumnTransformer (Impute/Encode/Scale)\n",
        "\n",
        "    IMPORTANT for sklearn.clone:\n",
        "    - __init__ stores params EXACTLY as received (no casting to list, no copies).\n",
        "    - All derived objects are created in fit().\n",
        "    \"\"\"\n",
        "    def __init__(self, num_cols, cat_cols, ord_num, ord_cat, covariates):\n",
        "        # Store raw params unchanged for sklearn.clone compatibility\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.ord_num = ord_num\n",
        "        self.ord_cat = ord_cat\n",
        "        self.covariates = covariates\n",
        "\n",
        "        # Will be built in fit()\n",
        "        self._resid = None\n",
        "        self._ct = None\n",
        "        self.feature_names_out_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Build residualizer fresh in fit (train fold only)\n",
        "        self._resid = ResidualizeCovariates(list(self.covariates) if self.covariates is not None else [])\n",
        "        Xr = self._resid.fit_transform(X)\n",
        "\n",
        "        # Build ColumnTransformer fresh in fit, using COPIES of column lists\n",
        "        ohe_kwargs = dict(handle_unknown=\"ignore\")\n",
        "        try:\n",
        "            # sklearn >=1.2\n",
        "            ohe_kwargs[\"sparse_output\"] = False\n",
        "        except Exception:\n",
        "            # older versions ignore this key; we'll set sparse=False instead below\n",
        "            pass\n",
        "\n",
        "        # Back-compat for pre-1.2\n",
        "        def _ohe():\n",
        "            try:\n",
        "                return OneHotEncoder(**ohe_kwargs)\n",
        "            except TypeError:\n",
        "                return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "        num_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scale\",  RobustScaler(with_centering=True))\n",
        "        ])\n",
        "        cat_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"ohe\",    _ohe())\n",
        "        ])\n",
        "        ord_num_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        ])\n",
        "        ord_cat_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"ordenc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "        ])\n",
        "\n",
        "        self._ct = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\",     num_block,  list(self.num_cols) if self.num_cols is not None else []),\n",
        "                (\"cat\",     cat_block,  list(self.cat_cols) if self.cat_cols is not None else []),\n",
        "                (\"ord_num\", ord_num_block, list(self.ord_num) if self.ord_num is not None else []),\n",
        "                (\"ord_cat\", ord_cat_block, list(self.ord_cat) if self.ord_cat is not None else []),\n",
        "            ],\n",
        "            remainder=\"drop\",\n",
        "            verbose_feature_names_out=False\n",
        "        )\n",
        "\n",
        "        Xt = self._ct.fit_transform(Xr)\n",
        "\n",
        "        # Cache feature names\n",
        "        try:\n",
        "            self.feature_names_out_ = self._ct.get_feature_names_out()\n",
        "        except Exception:\n",
        "            self.feature_names_out_ = np.array([f\"f{i}\" for i in range(Xt.shape[1])])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xr = self._resid.transform(X) if self._resid is not None else X\n",
        "        Xt = self._ct.transform(Xr)    if self._ct    is not None else Xr\n",
        "        return Xt\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.feature_names_out_\n",
        "\n",
        "# --- rebuild helper to instantiate the preprocessor\n",
        "def build_preprocessor() -> CombinedPreprocessor:\n",
        "    return CombinedPreprocessor(\n",
        "        num_cols=num_cols,      # use the variables you already defined above\n",
        "        cat_cols=cat_cols,\n",
        "        ord_num=ord_num,\n",
        "        ord_cat=ord_cat,\n",
        "        covariates=list(CFG.NUISANCE_COVARS)\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Model specs & imbalance strategies (all inside inner CV)\n",
        "# =========================\n",
        "def make_model_specs() -> List[Tuple[str, BaseEstimator, Dict, str]]:\n",
        "    specs = []\n",
        "\n",
        "    # Logistic Regression (cost-sensitive via class_weight)\n",
        "    lr = LogisticRegression(max_iter=2000, solver=\"saga\", penalty=\"l2\", random_state=CFG.RANDOM_STATE)\n",
        "    lr_grid = {\"clf__C\": np.logspace(-3, 2, 12)}\n",
        "    specs += [(\"LogReg\", lr, lr_grid, \"cost_sensitive\"),\n",
        "              (\"LogReg_SMOTE\", clone(lr), lr_grid, \"smote\"),\n",
        "              (\"LogReg_Under\", clone(lr), lr_grid, \"undersample\")]\n",
        "\n",
        "    # Random Forest (with cost-sensitive and sampling; BalancedRF is separate)\n",
        "    rf = RandomForestClassifier(n_estimators=400, random_state=CFG.RANDOM_STATE, n_jobs=-1)\n",
        "    rf_grid = {\n",
        "        \"clf__n_estimators\": [200, 400, 600],\n",
        "        \"clf__max_depth\": [None, 4, 6, 10],\n",
        "        \"clf__min_samples_split\": [2, 5, 10],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4]\n",
        "    }\n",
        "    specs += [(\"RF\", rf, rf_grid, \"cost_sensitive\"),\n",
        "              (\"RF_SMOTE\", clone(rf), rf_grid, \"smote\"),\n",
        "              (\"RF_Under\", clone(rf), rf_grid, \"undersample\")]\n",
        "\n",
        "    # Balanced Random Forest (ensemble balancing)\n",
        "    brf = BalancedRandomForestClassifier(n_estimators=400, random_state=CFG.RANDOM_STATE, n_jobs=-1)\n",
        "    brf_grid = {\"clf__n_estimators\": [200, 400, 600], \"clf__max_depth\": [None, 4, 6, 10]}\n",
        "    specs += [(\"BalancedRF\", brf, brf_grid, \"balanced_rf\")]\n",
        "\n",
        "    # EasyEnsemble (ensemble balancing via under-sampled AdaBoost)\n",
        "    eec = EasyEnsembleClassifier(\n",
        "        n_estimators=10, random_state=CFG.RANDOM_STATE, n_jobs=-1\n",
        "    )\n",
        "    eec_grid = {\"clf__n_estimators\": [6, 10, 14]}\n",
        "    specs += [(\"EasyEnsemble\", eec, eec_grid, \"ensemble_balance\")]  # imbalance handled internally\n",
        "\n",
        "    # XGBoost (include scale_pos_weight as proxy for cost-sensitivity)\n",
        "    if XGBClassifier is not None:\n",
        "        xgb = XGBClassifier(\n",
        "            objective=\"binary:logistic\", eval_metric=\"auc\",\n",
        "            random_state=CFG.RANDOM_STATE, tree_method=\"hist\",\n",
        "            n_estimators=400, n_jobs=-1\n",
        "        )\n",
        "        xgb_grid = {\n",
        "            \"clf__max_depth\": [3, 4, 5, 6],\n",
        "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 8),\n",
        "            \"clf__subsample\": [0.7, 0.85, 1.0],\n",
        "            \"clf__colsample_bytree\": [0.7, 0.85, 1.0],\n",
        "            \"clf__min_child_weight\": [1, 5, 10],\n",
        "            \"clf__scale_pos_weight\": [1, 2, 3, 4],  # simple grid; fold-specific ratio approximated\n",
        "        }\n",
        "        specs += [(\"XGB\", xgb, xgb_grid, \"cost_sensitive\"),\n",
        "                  (\"XGB_SMOTE\", clone(xgb), xgb_grid, \"smote\"),\n",
        "                  (\"XGB_Under\", clone(xgb), xgb_grid, \"undersample\")]\n",
        "\n",
        "    # LightGBM\n",
        "    if LGBMClassifier is not None:\n",
        "        lgb = LGBMClassifier(objective=\"binary\", random_state=CFG.RANDOM_STATE, n_estimators=600, n_jobs=-1)\n",
        "        lgb_grid = {\n",
        "            \"clf__num_leaves\": [15, 31, 63],\n",
        "            \"clf__max_depth\": [-1, 4, 6, 8],\n",
        "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 8),\n",
        "            \"clf__subsample\": [0.7, 0.85, 1.0],\n",
        "            \"clf__colsample_bytree\": [0.7, 0.85, 1.0],\n",
        "            \"clf__min_child_samples\": [5, 10, 20],\n",
        "            \"clf__class_weight\": [None, \"balanced\"]\n",
        "        }\n",
        "        specs += [(\"LGBM\", lgb, lgb_grid, \"cost_sensitive\"),\n",
        "                  (\"LGBM_SMOTE\", clone(lgb), lgb_grid, \"smote\"),\n",
        "                  (\"LGBM_Under\", clone(lgb), lgb_grid, \"undersample\")]\n",
        "\n",
        "    # CatBoost (works fine on dense arrays produced by CT)\n",
        "    if CatBoostClassifier is not None:\n",
        "        cb = CatBoostClassifier(\n",
        "            loss_function=\"Logloss\",\n",
        "            eval_metric=\"AUC\",\n",
        "            random_seed=CFG.RANDOM_STATE,\n",
        "            verbose=False,\n",
        "            iterations=600\n",
        "        )\n",
        "        cb_grid = {\n",
        "            \"clf__depth\": [4, 6, 8],\n",
        "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 6),\n",
        "            \"clf__l2_leaf_reg\": [1, 3, 5, 7, 9]\n",
        "        }\n",
        "        specs += [(\"CatBoost\", cb, cb_grid, \"cost_sensitive\")]  # CB lacks class_weight for dense; rely on params\n",
        "    return specs\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Build full pipeline for a given imbalance mode\n",
        "# IMPORTANT: All steps are inside the pipeline -> fit on training folds only.\n",
        "# =========================\n",
        "def build_pipeline(imbalance_mode: str, estimator) -> ImbPipeline:\n",
        "    preproc = build_preprocessor()\n",
        "    steps = [(\"prep\", preproc)]\n",
        "\n",
        "    if imbalance_mode == \"cost_sensitive\":\n",
        "        if hasattr(estimator, \"class_weight\"):\n",
        "            estimator = clone(estimator).set_params(class_weight=\"balanced\")\n",
        "        steps.append((\"clf\", estimator))\n",
        "\n",
        "    elif imbalance_mode == \"smote\":\n",
        "        steps += [(\"smote\", SMOTE(random_state=CFG.RANDOM_STATE)), (\"clf\", estimator)]\n",
        "\n",
        "    elif imbalance_mode == \"undersample\":\n",
        "        steps += [(\"under\", RandomUnderSampler(random_state=CFG.RANDOM_STATE)), (\"clf\", estimator)]\n",
        "\n",
        "    elif imbalance_mode == \"balanced_rf\":\n",
        "        steps.append((\"clf\", estimator))  # BRF handles rebalancing internally\n",
        "\n",
        "    elif imbalance_mode == \"ensemble_balance\":   # EasyEnsemble\n",
        "        steps.append((\"clf\", estimator))          # internal under-sampled ensembles\n",
        "\n",
        "    else:\n",
        "        steps.append((\"clf\", estimator))          # fallback\n",
        "\n",
        "    return ImbPipeline(steps)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Nested CV (outer evaluation / inner tuning) — NeuroMiner-style\n",
        "# =========================\n",
        "def run_nested_cv(X: pd.DataFrame, y: np.ndarray) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    outer = StratifiedKFold(n_splits=CFG.OUTER_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
        "    rows = []\n",
        "    specs = make_model_specs()\n",
        "\n",
        "    for model_name, est, grid, imb_mode in specs:\n",
        "        print(f\"\\n=== {model_name} | {imb_mode} ===\")\n",
        "        ofold = 0\n",
        "        for tr, te in outer.split(X, y):\n",
        "            ofold += 1\n",
        "            X_tr, X_te = X.iloc[tr].copy(), X.iloc[te].copy()\n",
        "            y_tr, y_te = y[tr], y[te]\n",
        "\n",
        "            pipe = build_pipeline(imb_mode, est)\n",
        "            inner = StratifiedKFold(n_splits=CFG.INNER_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
        "\n",
        "            # Hyperparameter search **inside** inner CV (no peek at outer test) -> NM-compliant\n",
        "            search = RandomizedSearchCV(\n",
        "                estimator=pipe,\n",
        "                param_distributions=grid,\n",
        "                n_iter=CFG.N_TRIALS,\n",
        "                scoring=\"roc_auc\",\n",
        "                cv=inner,\n",
        "                n_jobs=-1,\n",
        "                refit=True,\n",
        "                random_state=CFG.RANDOM_STATE,\n",
        "                verbose=0\n",
        "            )\n",
        "            t0 = time.time()\n",
        "            search.fit(X_tr, y_tr)\n",
        "            t_elapsed = round(time.time() - t0, 2)\n",
        "\n",
        "            best = search.best_estimator_\n",
        "            if hasattr(best, \"predict_proba\"):\n",
        "                y_prob = best.predict_proba(X_te)[:,1]\n",
        "            else:\n",
        "                y_prob = best.decision_function(X_te)\n",
        "\n",
        "            metrics = compute_metrics(y_te, y_prob, thr=0.5)\n",
        "            rows.append({\n",
        "                \"Model\": model_name, \"Imbalance\": imb_mode, \"OuterFold\": ofold,\n",
        "                \"BestParams\": json.dumps(search.best_params_), \"SearchTimeSec\": t_elapsed, **metrics\n",
        "            })\n",
        "\n",
        "    fold_df = pd.DataFrame(rows)\n",
        "    summary = (fold_df\n",
        "               .groupby([\"Model\",\"Imbalance\"], as_index=False)\n",
        "               .agg(N=(\"OuterFold\",\"count\"),\n",
        "                    Sensitivity_mean=(\"Sensitivity\",\"mean\"), Sensitivity_sd=(\"Sensitivity\",\"std\"),\n",
        "                    Specificity_mean=(\"Specificity\",\"mean\"), Specificity_sd=(\"Specificity\",\"std\"),\n",
        "                    BAC_mean=(\"BAC\",\"mean\"), BAC_sd=(\"BAC\",\"std\"),\n",
        "                    ROC_AUC_mean=(\"ROC_AUC\",\"mean\"), ROC_AUC_sd=(\"ROC_AUC\",\"std\"),\n",
        "                    PR_AUC_mean=(\"PR_AUC\",\"mean\"), PR_AUC_sd=(\"PR_AUC\",\"std\"),\n",
        "                    SearchTimeSec_sum=(\"SearchTimeSec\",\"sum\")))\n",
        "    return fold_df, summary\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Run (reduced folds for Oct 20 brief)\n",
        "# =========================\n",
        "fold_df, summary_df = run_nested_cv(X, y)\n",
        "\n",
        "out1 = os.path.join(CFG.RESULTS_DIR, \"nm_fold_metrics.csv\")\n",
        "out2 = os.path.join(CFG.RESULTS_DIR, \"nm_summary_by_model.csv\")\n",
        "fold_df.to_csv(out1, index=False); summary_df.to_csv(out2, index=False)\n",
        "\n",
        "print(\"\\n[DONE] Nested CV complete (NM-style).\")\n",
        "print(f\"Saved -> {out1}\\n      -> {out2}\")\n",
        "\n",
        "# quick preview of top performers by PR-AUC then BAC (for the brief)\n",
        "preview = summary_df.assign(_rk=summary_df[\"PR_AUC_mean\"].fillna(-1) + 1e-3*summary_df[\"BAC_mean\"].fillna(-1)) \\\n",
        "                    .sort_values(\"_rk\", ascending=False) \\\n",
        "                    .drop(columns=[\"_rk\"])\n",
        "display(preview.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "rfd_WMjWpz7a",
        "outputId": "bc0fc8c5-8858-4424-cc99-92fd2fe20588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "           Model         Imbalance  N  Sensitivity_mean  Sensitivity_sd  \\\n",
              "12     XGB_SMOTE             smote  2            0.3750        0.000000   \n",
              "11           XGB    cost_sensitive  2            0.3750        0.176777   \n",
              "6   LogReg_SMOTE             smote  2            0.5000        0.176777   \n",
              "8             RF    cost_sensitive  2            0.0000        0.000000   \n",
              "5         LogReg    cost_sensitive  2            0.4375        0.265165   \n",
              "9       RF_SMOTE             smote  2            0.0000        0.000000   \n",
              "0     BalancedRF       balanced_rf  2            0.3125        0.088388   \n",
              "1   EasyEnsemble  ensemble_balance  2            0.3750        0.176777   \n",
              "2           LGBM    cost_sensitive  2            0.1250        0.000000   \n",
              "4     LGBM_Under       undersample  2            0.6250        0.530330   \n",
              "\n",
              "    Specificity_mean  Specificity_sd  BAC_mean    BAC_sd  ROC_AUC_mean  \\\n",
              "12             0.850        0.141421   0.61250  0.070711      0.665625   \n",
              "11             0.900        0.070711   0.63750  0.053033      0.721875   \n",
              "6              0.800        0.070711   0.65000  0.053033      0.625000   \n",
              "8              0.975        0.035355   0.48750  0.017678      0.737500   \n",
              "5              0.800        0.070711   0.61875  0.097227      0.600000   \n",
              "9              0.975        0.035355   0.48750  0.017678      0.709375   \n",
              "0              0.900        0.000000   0.60625  0.044194      0.723437   \n",
              "1              0.800        0.070711   0.58750  0.123744      0.609375   \n",
              "2              0.975        0.035355   0.55000  0.017678      0.593750   \n",
              "4              0.450        0.636396   0.53750  0.053033      0.581250   \n",
              "\n",
              "    ROC_AUC_sd  PR_AUC_mean  PR_AUC_sd  SearchTimeSec_sum  \n",
              "12    0.075130     0.574094   0.099239               4.04  \n",
              "11    0.030936     0.567401   0.030195               5.49  \n",
              "6     0.044194     0.548468   0.077877               4.73  \n",
              "8     0.053033     0.540357   0.006501              30.33  \n",
              "5     0.070711     0.512928   0.115510               4.23  \n",
              "9     0.013258     0.504167   0.048614              14.83  \n",
              "0     0.050823     0.496487   0.004400              34.56  \n",
              "1     0.110485     0.477195   0.220426              17.98  \n",
              "2     0.088388     0.426060   0.026332              11.69  \n",
              "4     0.114905     0.421296   0.191741               6.85  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ea77fe18-5211-4464-b731-ca6bc69f2027\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Imbalance</th>\n",
              "      <th>N</th>\n",
              "      <th>Sensitivity_mean</th>\n",
              "      <th>Sensitivity_sd</th>\n",
              "      <th>Specificity_mean</th>\n",
              "      <th>Specificity_sd</th>\n",
              "      <th>BAC_mean</th>\n",
              "      <th>BAC_sd</th>\n",
              "      <th>ROC_AUC_mean</th>\n",
              "      <th>ROC_AUC_sd</th>\n",
              "      <th>PR_AUC_mean</th>\n",
              "      <th>PR_AUC_sd</th>\n",
              "      <th>SearchTimeSec_sum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>XGB_SMOTE</td>\n",
              "      <td>smote</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.850</td>\n",
              "      <td>0.141421</td>\n",
              "      <td>0.61250</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.665625</td>\n",
              "      <td>0.075130</td>\n",
              "      <td>0.574094</td>\n",
              "      <td>0.099239</td>\n",
              "      <td>4.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>XGB</td>\n",
              "      <td>cost_sensitive</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.63750</td>\n",
              "      <td>0.053033</td>\n",
              "      <td>0.721875</td>\n",
              "      <td>0.030936</td>\n",
              "      <td>0.567401</td>\n",
              "      <td>0.030195</td>\n",
              "      <td>5.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LogReg_SMOTE</td>\n",
              "      <td>smote</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.65000</td>\n",
              "      <td>0.053033</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.044194</td>\n",
              "      <td>0.548468</td>\n",
              "      <td>0.077877</td>\n",
              "      <td>4.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>RF</td>\n",
              "      <td>cost_sensitive</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.975</td>\n",
              "      <td>0.035355</td>\n",
              "      <td>0.48750</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.053033</td>\n",
              "      <td>0.540357</td>\n",
              "      <td>0.006501</td>\n",
              "      <td>30.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LogReg</td>\n",
              "      <td>cost_sensitive</td>\n",
              "      <td>2</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.265165</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.61875</td>\n",
              "      <td>0.097227</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.512928</td>\n",
              "      <td>0.115510</td>\n",
              "      <td>4.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>RF_SMOTE</td>\n",
              "      <td>smote</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.975</td>\n",
              "      <td>0.035355</td>\n",
              "      <td>0.48750</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.709375</td>\n",
              "      <td>0.013258</td>\n",
              "      <td>0.504167</td>\n",
              "      <td>0.048614</td>\n",
              "      <td>14.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BalancedRF</td>\n",
              "      <td>balanced_rf</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3125</td>\n",
              "      <td>0.088388</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.60625</td>\n",
              "      <td>0.044194</td>\n",
              "      <td>0.723437</td>\n",
              "      <td>0.050823</td>\n",
              "      <td>0.496487</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>34.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>EasyEnsemble</td>\n",
              "      <td>ensemble_balance</td>\n",
              "      <td>2</td>\n",
              "      <td>0.3750</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.070711</td>\n",
              "      <td>0.58750</td>\n",
              "      <td>0.123744</td>\n",
              "      <td>0.609375</td>\n",
              "      <td>0.110485</td>\n",
              "      <td>0.477195</td>\n",
              "      <td>0.220426</td>\n",
              "      <td>17.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LGBM</td>\n",
              "      <td>cost_sensitive</td>\n",
              "      <td>2</td>\n",
              "      <td>0.1250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.975</td>\n",
              "      <td>0.035355</td>\n",
              "      <td>0.55000</td>\n",
              "      <td>0.017678</td>\n",
              "      <td>0.593750</td>\n",
              "      <td>0.088388</td>\n",
              "      <td>0.426060</td>\n",
              "      <td>0.026332</td>\n",
              "      <td>11.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LGBM_Under</td>\n",
              "      <td>undersample</td>\n",
              "      <td>2</td>\n",
              "      <td>0.6250</td>\n",
              "      <td>0.530330</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.636396</td>\n",
              "      <td>0.53750</td>\n",
              "      <td>0.053033</td>\n",
              "      <td>0.581250</td>\n",
              "      <td>0.114905</td>\n",
              "      <td>0.421296</td>\n",
              "      <td>0.191741</td>\n",
              "      <td>6.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ea77fe18-5211-4464-b731-ca6bc69f2027')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ea77fe18-5211-4464-b731-ca6bc69f2027 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ea77fe18-5211-4464-b731-ca6bc69f2027');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7867e241-820c-420d-be33-77311aa6d515\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7867e241-820c-420d-be33-77311aa6d515')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7867e241-820c-420d-be33-77311aa6d515 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(preview\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"LGBM\",\n          \"XGB\",\n          \"RF_SMOTE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Imbalance\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"cost_sensitive\",\n          \"undersample\",\n          \"balanced_rf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"N\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20833333333333334,\n        \"min\": 0.0,\n        \"max\": 0.625,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1677050983124842,\n        \"min\": 0.0,\n        \"max\": 0.5303300858899106,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1767766952966369\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15592466129512675,\n        \"min\": 0.45,\n        \"max\": 0.975,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18637626219857267,\n        \"min\": 0.0,\n        \"max\": 0.6363961030678927,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.14142135623730948\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BAC_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05894029464918999,\n        \"min\": 0.4875,\n        \"max\": 0.65,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.55\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BAC_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03505947327993772,\n        \"min\": 0.017677669529663664,\n        \"max\": 0.12374368670764585,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.05303300858899115\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROC_AUC_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06164365596864412,\n        \"min\": 0.58125,\n        \"max\": 0.7375,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.59375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROC_AUC_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0330923909200522,\n        \"min\": 0.013258252147247797,\n        \"max\": 0.11490485194281391,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.08838834764831832\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PR_AUC_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05366606962700845,\n        \"min\": 0.4212956712405242,\n        \"max\": 0.5740941368572947,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.4260603300684822\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PR_AUC_sd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0754696065300187,\n        \"min\": 0.00439954560600295,\n        \"max\": 0.22042561632686167,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.026332449883681403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SearchTimeSec_sum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.118556111294309,\n        \"min\": 4.04,\n        \"max\": 34.56,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          11.69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# NeuroMiner-compliant nested CV with permutation tests\n",
        "# =========================\n",
        "import os, json, time, warnings, numpy as np, pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# sklearn / imblearn\n",
        "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "\n",
        "# Boosters (conditional)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except Exception:\n",
        "    XGBClassifier = None\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "\n",
        "# Tabular deep models (conditional)\n",
        "try:\n",
        "    from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "except Exception:\n",
        "    TabNetClassifier = None\n",
        "\n",
        "# A minimal TabTransformer via pytorch-widedeep (optional)\n",
        "try:\n",
        "    from pytorch_widedeep.models import TabTransformer\n",
        "    from pytorch_widedeep import Trainer, TabularDataset\n",
        "    TABTRANSFORMER_AVAILABLE = True\n",
        "except Exception:\n",
        "    TABTRANSFORMER_AVAILABLE = False\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    DATA_PATH: str = \"/content/Final Dastaset Oct25.xlsx\"\n",
        "    TARGET_NAME_PREFERRED: str = \"Tendinopathy Injury\"\n",
        "    TARGET_EXCEL_LETTER: str = \"BI\"\n",
        "    ORDINAL_RANGES: Tuple[str, ...] = (\"AH:AO\", \"AQ:BG\")\n",
        "    RANDOM_STATE: int = 42\n",
        "    OUTER_FOLDS: int = 2      # use 5 for final\n",
        "    INNER_FOLDS: int = 3      # use 5 for final\n",
        "    N_TRIALS: int = 5         # increase for final\n",
        "    RESULTS_DIR: str = \"./results_nm_full\"\n",
        "    NUISANCE_COVARS: Tuple[str, ...] = (\"Age\",)\n",
        "    MISSING_THRESHOLD: float = 0.8\n",
        "    N_PERMUTATIONS: int = 50  # raise for final report if feasible\n",
        "\n",
        "CFG = Config()\n",
        "os.makedirs(CFG.RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utilities / helpers\n",
        "# -------------------------\n",
        "def excel_letter_to_index(letter: str) -> int:\n",
        "    letter = letter.strip().upper()\n",
        "    n=0\n",
        "    for ch in letter:\n",
        "        n = n*26 + (ord(ch)-64)\n",
        "    return n-1\n",
        "\n",
        "def expand_excel_range_to_names(df: pd.DataFrame, excel_range: str) -> List[str]:\n",
        "    a,b = [s.strip().upper() for s in excel_range.split(\":\")]\n",
        "    i1,i2 = excel_letter_to_index(a), excel_letter_to_index(b)\n",
        "    lo,hi = min(i1,i2), max(i1,i2)\n",
        "    cols = df.columns.tolist()\n",
        "    hi = min(hi, len(cols)-1); lo=max(lo,0)\n",
        "    return cols[lo:hi+1]\n",
        "\n",
        "def binarize_yes_no(series: pd.Series) -> np.ndarray:\n",
        "    mapped = series.astype(str).str.strip().str.lower().map({\"yes\":1,\"no\":0})\n",
        "    keep = mapped.isin([0,1])\n",
        "    return mapped[keep].astype(int).values, keep.values\n",
        "\n",
        "def specificity_score(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn/(tn+fp) if (tn+fp)>0 else 0.0\n",
        "\n",
        "def compute_metrics(y_true, y_prob, thr=0.5) -> Dict[str,float]:\n",
        "    y_pred = (y_prob >= thr).astype(int)\n",
        "    sens = recall_score(y_true, y_pred)\n",
        "    spec = specificity_score(y_true, y_pred)\n",
        "    bac  = 0.5*(sens+spec)\n",
        "    roc  = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
        "    pr   = average_precision_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan\n",
        "    return {\"Sensitivity\": sens, \"Specificity\": spec, \"BAC\": bac, \"ROC_AUC\": roc, \"PR_AUC\": pr}\n",
        "\n",
        "# -------------------------\n",
        "# Residualizer (NaN-safe)\n",
        "# -------------------------\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "class ResidualizeCovariates(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, covariate_cols):\n",
        "        self.covariate_cols = covariate_cols\n",
        "        self.models_ = {}\n",
        "        self.cov_medians_ = None\n",
        "        self.feature_cols_ = None\n",
        "        self.covars_present_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if not hasattr(X, \"columns\"):\n",
        "            self.feature_cols_ = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "            self.covars_present_ = []\n",
        "            return self\n",
        "        self.feature_cols_ = list(X.columns)\n",
        "        self.covars_present_ = [c for c in (self.covariate_cols or []) if c in X.columns]\n",
        "        if not self.covars_present_:\n",
        "            return self\n",
        "\n",
        "        C = X[self.covars_present_].to_numpy()\n",
        "        C_finite = np.isfinite(C).all(axis=1)\n",
        "        self.cov_medians_ = np.nanmedian(C[C_finite], axis=0) if C_finite.any() else np.nanmedian(C, axis=0)\n",
        "\n",
        "        for col in self.feature_cols_:\n",
        "            if col in self.covars_present_:\n",
        "                continue\n",
        "            xi = X[col].to_numpy()\n",
        "            if not np.issubdtype(np.asarray(xi).dtype, np.number):\n",
        "                continue\n",
        "            mask = np.isfinite(xi) & np.isfinite(C).all(axis=1)\n",
        "            if mask.sum() < 5:\n",
        "                continue\n",
        "            lr = LinearRegression()\n",
        "            lr.fit(C[mask], xi[mask])\n",
        "            self.models_[col] = lr\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if not hasattr(X, \"copy\") or not self.models_:\n",
        "            return X\n",
        "        Xr = X.copy()\n",
        "        if self.covars_present_:\n",
        "            C = Xr[self.covars_present_].to_numpy()\n",
        "            C_imp = C.copy()\n",
        "            med = self.cov_medians_ if self.cov_medians_ is not None else np.nanmedian(C_imp, axis=0)\n",
        "            nan_mask = ~np.isfinite(C_imp)\n",
        "            if nan_mask.any():\n",
        "                C_imp[nan_mask] = np.take(med, np.where(nan_mask)[1])\n",
        "            for col, lr in self.models_.items():\n",
        "                if col in Xr.columns:\n",
        "                    xi = Xr[col].to_numpy()\n",
        "                    pred = lr.predict(C_imp)\n",
        "                    finite = np.isfinite(xi)\n",
        "                    xi_resid = xi.copy()\n",
        "                    xi_resid[finite] = xi[finite] - pred[finite]\n",
        "                    Xr[col] = xi_resid\n",
        "        return Xr\n",
        "\n",
        "# -------------------------\n",
        "# Combined Preprocessor (single transformer for imblearn.Pipeline)\n",
        "# -------------------------\n",
        "class CombinedPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, num_cols, cat_cols, ord_num, ord_cat, covariates):\n",
        "        self.num_cols = num_cols\n",
        "        self.cat_cols = cat_cols\n",
        "        self.ord_num = ord_num\n",
        "        self.ord_cat = ord_cat\n",
        "        self.covariates = covariates\n",
        "        self._resid = None\n",
        "        self._ct = None\n",
        "        self.feature_names_out_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self._resid = ResidualizeCovariates(list(self.covariates) if self.covariates is not None else [])\n",
        "        Xr = self._resid.fit_transform(X)\n",
        "\n",
        "        def _ohe():\n",
        "            try:\n",
        "                return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "            except TypeError:\n",
        "                return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "\n",
        "        num_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
        "            (\"scale\",  RobustScaler(with_centering=True)),\n",
        "        ])\n",
        "        cat_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"ohe\", _ohe())\n",
        "        ])\n",
        "        ord_num_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        ])\n",
        "        ord_cat_block = Pipeline([\n",
        "            (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"ordenc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "        ])\n",
        "\n",
        "        self._ct = ColumnTransformer(\n",
        "            transformers=[\n",
        "                (\"num\",     num_block,  list(self.num_cols) if self.num_cols is not None else []),\n",
        "                (\"cat\",     cat_block,  list(self.cat_cols) if self.cat_cols is not None else []),\n",
        "                (\"ord_num\", ord_num_block, list(self.ord_num) if self.ord_num is not None else []),\n",
        "                (\"ord_cat\", ord_cat_block, list(self.ord_cat) if self.ord_cat is not None else []),\n",
        "            ],\n",
        "            remainder=\"drop\",\n",
        "            verbose_feature_names_out=False\n",
        "        )\n",
        "        Xt = self._ct.fit_transform(Xr)\n",
        "        try:\n",
        "            self.feature_names_out_ = self._ct.get_feature_names_out()\n",
        "        except Exception:\n",
        "            self.feature_names_out_ = np.array([f\"f{i}\" for i in range(Xt.shape[1])])\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        Xr = self._resid.transform(X) if self._resid is not None else X\n",
        "        Xt = self._ct.transform(Xr) if self._ct is not None else Xr\n",
        "        return Xt\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.feature_names_out_\n",
        "\n",
        "# -------------------------\n",
        "# Load data\n",
        "# -------------------------\n",
        "assert os.path.exists(CFG.DATA_PATH), f\"File not found: {CFG.DATA_PATH}\"\n",
        "df = pd.read_excel(CFG.DATA_PATH)\n",
        "\n",
        "# Target column\n",
        "if CFG.TARGET_NAME_PREFERRED in df.columns:\n",
        "    target_col = CFG.TARGET_NAME_PREFERRED\n",
        "else:\n",
        "    target_col = df.columns[excel_letter_to_index(CFG.TARGET_EXCEL_LETTER)]\n",
        "\n",
        "# Drop rows with missing target\n",
        "df = df.dropna(subset=[target_col]).copy()\n",
        "\n",
        "# Drop potential leakage/meta\n",
        "drop_cols = [\n",
        "    'Date of Injury','Location','Affected tendon','Leg./arm Injured',\n",
        "    'Injury severity','If other, please write',\n",
        "    'No. of partial games participation before injury',\n",
        "    'No. of training sessions before injury',\n",
        "    'No. of full games participation before injury',\n",
        "    'Code'\n",
        "]\n",
        "df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
        "\n",
        "# Drop columns with too much missingness\n",
        "miss = df.isnull().sum()/len(df)\n",
        "df = df.drop(columns=miss[miss>CFG.MISSING_THRESHOLD].index.tolist(), errors='ignore')\n",
        "\n",
        "# Ordinals from Excel ranges\n",
        "ordinal_cols = []\n",
        "for r in CFG.ORDINAL_RANGES:\n",
        "    ordinal_cols += [c for c in expand_excel_range_to_names(df, r) if c != target_col and c in df.columns]\n",
        "ordinal_cols = list(dict.fromkeys(ordinal_cols))\n",
        "\n",
        "# y and X\n",
        "y_full, keep_mask = binarize_yes_no(df[target_col])\n",
        "X_full = df.loc[keep_mask, :].drop(columns=[target_col]).copy()\n",
        "\n",
        "num_cols  = [c for c in X_full.select_dtypes(include=[np.number]).columns if c not in ordinal_cols]\n",
        "cat_cols  = [c for c in X_full.select_dtypes(exclude=[np.number]).columns if c not in ordinal_cols]\n",
        "ord_num   = [c for c in ordinal_cols if c in X_full.columns and pd.api.types.is_numeric_dtype(X_full[c])]\n",
        "ord_cat   = [c for c in ordinal_cols if c in X_full.columns and not pd.api.types.is_numeric_dtype(X_full[c])]\n",
        "\n",
        "# -------------------------\n",
        "# Preprocessor factory (no fitting here)\n",
        "# -------------------------\n",
        "def build_preprocessor() -> CombinedPreprocessor:\n",
        "    return CombinedPreprocessor(\n",
        "        num_cols=num_cols, cat_cols=cat_cols, ord_num=ord_num, ord_cat=ord_cat,\n",
        "        covariates=list(CFG.NUISANCE_COVARS)\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# Model specs (all inside CV2)\n",
        "# -------------------------\n",
        "def make_model_specs() -> List[Tuple[str, BaseEstimator, Dict, str]]:\n",
        "    specs: List[Tuple[str, BaseEstimator, Dict, str]] = []\n",
        "\n",
        "    # Logistic Regression\n",
        "    lr = LogisticRegression(max_iter=3000, solver=\"saga\", penalty=\"l2\", random_state=CFG.RANDOM_STATE)\n",
        "    lr_grid = {\"clf__C\": np.logspace(-3, 2, 12)}\n",
        "    specs += [\n",
        "        (\"LogReg\", lr, lr_grid, \"cost_sensitive\"),\n",
        "        (\"LogReg_SMOTE\", clone(lr), lr_grid, \"smote\"),\n",
        "        (\"LogReg_Under\", clone(lr), lr_grid, \"undersample\"),\n",
        "    ]\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=400, random_state=CFG.RANDOM_STATE, n_jobs=-1)\n",
        "    rf_grid = {\n",
        "        \"clf__n_estimators\": [200, 400, 600],\n",
        "        \"clf__max_depth\": [None, 3, 5, 8],\n",
        "        \"clf__min_samples_split\": [2, 5, 10],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "    }\n",
        "    specs += [\n",
        "        (\"RF\", rf, rf_grid, \"cost_sensitive\"),\n",
        "        (\"RF_SMOTE\", clone(rf), rf_grid, \"smote\"),\n",
        "        (\"RF_Under\", clone(rf), rf_grid, \"undersample\"),\n",
        "    ]\n",
        "\n",
        "    # Balanced Random Forest\n",
        "    brf = BalancedRandomForestClassifier(n_estimators=400, random_state=CFG.RANDOM_STATE, n_jobs=-1)\n",
        "    brf_grid = {\"clf__n_estimators\": [200, 400, 600], \"clf__max_depth\": [None, 3, 5, 8]}\n",
        "    specs += [(\"BalancedRF\", brf, brf_grid, \"balanced_rf\")]\n",
        "\n",
        "    # EasyEnsemble\n",
        "    eec = EasyEnsembleClassifier(n_estimators=10, random_state=CFG.RANDOM_STATE, n_jobs=-1)\n",
        "    eec_grid = {\"clf__n_estimators\": [6, 10, 14]}\n",
        "    specs += [(\"EasyEnsemble\", eec, eec_grid, \"ensemble_balance\")]\n",
        "\n",
        "    # XGBoost\n",
        "    if XGBClassifier is not None:\n",
        "        xgb = XGBClassifier(\n",
        "            objective=\"binary:logistic\", eval_metric=\"auc\",\n",
        "            random_state=CFG.RANDOM_STATE, tree_method=\"hist\",\n",
        "            n_estimators=400, n_jobs=-1\n",
        "        )\n",
        "        xgb_grid = {\n",
        "            \"clf__max_depth\": [3, 4, 5, 6],\n",
        "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 8),\n",
        "            \"clf__subsample\": [0.7, 0.9, 1.0],\n",
        "            \"clf__colsample_bytree\": [0.7, 0.9, 1.0],\n",
        "            \"clf__min_child_weight\": [1, 5, 10],\n",
        "            \"clf__scale_pos_weight\": [1, 2, 3, 4],\n",
        "        }\n",
        "        specs += [\n",
        "            (\"XGB\", xgb, xgb_grid, \"cost_sensitive\"),\n",
        "            (\"XGB_SMOTE\", clone(xgb), xgb_grid, \"smote\"),\n",
        "            (\"XGB_Under\", clone(xgb), xgb_grid, \"undersample\"),\n",
        "        ]\n",
        "\n",
        "    # LightGBM\n",
        "    if LGBMClassifier is not None:\n",
        "        lgb = LGBMClassifier(\n",
        "            objective=\"binary\", random_state=CFG.RANDOM_STATE,\n",
        "            n_estimators=600, n_jobs=-1, verbose=-1\n",
        "        )\n",
        "        lgb_grid = {\n",
        "            \"clf__num_leaves\": [7, 15, 31],\n",
        "            \"clf__max_depth\": [-1, 3, 5],\n",
        "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"clf__subsample\": [0.7, 0.9, 1.0],\n",
        "            \"clf__colsample_bytree\": [0.7, 0.9, 1.0],\n",
        "            \"clf__min_child_samples\": [5, 10, 20],\n",
        "            \"clf__min_split_gain\": [0.0, 0.01],\n",
        "            \"clf__class_weight\": [None, \"balanced\"],\n",
        "        }\n",
        "        specs += [\n",
        "            (\"LGBM\", lgb, lgb_grid, \"cost_sensitive\"),\n",
        "            (\"LGBM_SMOTE\", clone(lgb), lgb_grid, \"smote\"),\n",
        "            (\"LGBM_Under\", clone(lgb), lgb_grid, \"undersample\"),\n",
        "        ]\n",
        "\n",
        "    # CatBoost\n",
        "    if CatBoostClassifier is not None:\n",
        "        cb = CatBoostClassifier(\n",
        "            loss_function=\"Logloss\", eval_metric=\"AUC\",\n",
        "            random_seed=CFG.RANDOM_STATE, verbose=False, iterations=600\n",
        "        )\n",
        "        cb_grid = {\n",
        "            \"clf__depth\": [4, 6, 8],\n",
        "            \"clf__learning_rate\": np.linspace(0.01, 0.2, 6),\n",
        "            \"clf__l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
        "        }\n",
        "        specs += [(\"CatBoost\", cb, cb_grid, \"cost_sensitive\")]\n",
        "\n",
        "    # TabNet (if available)\n",
        "    if TabNetClassifier is not None:\n",
        "        tabnet = TabNetClassifier(seed=CFG.RANDOM_STATE, verbose=0)\n",
        "        tn_grid = {\n",
        "            \"clf__n_d\": [8, 16], \"clf__n_a\": [8, 16], \"clf__n_steps\": [3, 4],\n",
        "            \"clf__gamma\": [1.0, 1.5], \"clf__lambda_sparse\": [0.0, 1e-4]\n",
        "        }\n",
        "        specs += [\n",
        "            (\"TabNet_SMOTE\", tabnet, tn_grid, \"smote\"),\n",
        "            (\"TabNet_Under\", clone(tabnet), tn_grid, \"undersample\"),\n",
        "        ]\n",
        "\n",
        "    # --------- Hybrid Stacking Meta-learner (INSIDE the function) ---------\n",
        "    base_estimators = [\n",
        "        (\"lr\", LogisticRegression(max_iter=2000, solver=\"saga\", penalty=\"l2\")),\n",
        "        (\"rf\", RandomForestClassifier(n_estimators=300, random_state=CFG.RANDOM_STATE)),\n",
        "    ]\n",
        "    if XGBClassifier is not None:\n",
        "        base_estimators.append(\n",
        "            (\"xgb\", XGBClassifier(objective=\"binary:logistic\", eval_metric=\"auc\",\n",
        "                                  random_state=CFG.RANDOM_STATE, tree_method=\"hist\",\n",
        "                                  n_estimators=300, n_jobs=-1))\n",
        "        )\n",
        "    if LGBMClassifier is not None:\n",
        "        base_estimators.append(\n",
        "            (\"lgb\", LGBMClassifier(objective=\"binary\", random_state=CFG.RANDOM_STATE,\n",
        "                                   n_estimators=400, n_jobs=-1, verbose=-1))\n",
        "        )\n",
        "    if len(base_estimators) >= 2:\n",
        "        stack = StackingClassifier(\n",
        "            estimators=base_estimators,\n",
        "            final_estimator=LogisticRegression(max_iter=2000, solver=\"lbfgs\"),\n",
        "            n_jobs=-1, passthrough=False\n",
        "        )\n",
        "        # Small but valid grid on the meta-learner\n",
        "        stack_grid = {\n",
        "            \"clf__final_estimator__C\": [0.25, 1.0, 4.0],\n",
        "            \"clf__final_estimator__class_weight\": [None, \"balanced\"],\n",
        "        }\n",
        "        specs += [(\"Stacking\", stack, stack_grid, \"cost_sensitive\")]\n",
        "\n",
        "    return specs\n",
        "\n",
        "# -------------------------\n",
        "# Build pipeline by imbalance mode\n",
        "# -------------------------\n",
        "def build_pipeline(imbalance_mode: str, estimator) -> ImbPipeline:\n",
        "    preproc = build_preprocessor()\n",
        "    steps = [(\"prep\", preproc)]\n",
        "    if imbalance_mode == \"cost_sensitive\":\n",
        "        if hasattr(estimator, \"class_weight\"):\n",
        "            estimator = clone(estimator).set_params(class_weight=\"balanced\")\n",
        "        steps.append((\"clf\", estimator))\n",
        "    elif imbalance_mode == \"smote\":\n",
        "        steps += [(\"smote\", SMOTE(random_state=CFG.RANDOM_STATE)), (\"clf\", estimator)]\n",
        "    elif imbalance_mode == \"undersample\":\n",
        "        steps += [(\"under\", RandomUnderSampler(random_state=CFG.RANDOM_STATE)), (\"clf\", estimator)]\n",
        "    elif imbalance_mode in (\"balanced_rf\", \"ensemble_balance\"):\n",
        "        steps.append((\"clf\", estimator))  # internal balancing\n",
        "    else:\n",
        "        steps.append((\"clf\", estimator))\n",
        "    return ImbPipeline(steps)\n",
        "\n",
        "# -------------------------\n",
        "# INNER CV: tune inside CV1-train (returns best params + best score)\n",
        "# -------------------------\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "def inner_cv(X_tr, y_tr, model_name, estimator, grid, imb_mode, random_state):\n",
        "    pipe = build_pipeline(imb_mode, estimator)\n",
        "    inner = StratifiedKFold(n_splits=CFG.INNER_FOLDS, shuffle=True, random_state=random_state)\n",
        "\n",
        "    if not grid:  # e.g., StackingClassifier with no search space\n",
        "        search = GridSearchCV(\n",
        "            estimator=pipe,\n",
        "            param_grid={},          # single fit\n",
        "            scoring=\"roc_auc\",\n",
        "            cv=inner,\n",
        "            n_jobs=-1,\n",
        "            refit=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    else:\n",
        "        search = RandomizedSearchCV(\n",
        "            estimator=pipe,\n",
        "            param_distributions=grid,\n",
        "            n_iter=CFG.N_TRIALS,\n",
        "            scoring=\"roc_auc\",\n",
        "            cv=inner,\n",
        "            n_jobs=-1,\n",
        "            refit=True,\n",
        "            random_state=random_state,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    search.fit(X_tr, y_tr)\n",
        "    return search.best_params_, search.best_score_\n",
        "\n",
        "# -------------------------\n",
        "# OUTER CV: orchestrate NM nesting & evaluation\n",
        "# -------------------------\n",
        "def outer_cv(X: pd.DataFrame, y: np.ndarray) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    rng = check_random_state(CFG.RANDOM_STATE)\n",
        "    outer = StratifiedKFold(n_splits=CFG.OUTER_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
        "    rows = []\n",
        "    specs = make_model_specs()\n",
        "\n",
        "    for model_name, est, grid, imb_mode in specs:\n",
        "        print(f\"\\n=== {model_name} | {imb_mode} ===\")\n",
        "        for ofold, (tr, te) in enumerate(outer.split(X, y), start=1):\n",
        "            X_tr, X_te = X.iloc[tr].copy(), X.iloc[te].copy()\n",
        "            y_tr, y_te = y[tr], y[te]\n",
        "            t0 = time.time()\n",
        "            best_params, inner_auc = inner_cv(X_tr, y_tr, model_name, est, grid, imb_mode, random_state=rng.randint(0,1_000_000))\n",
        "            # Rebuild best pipeline, refit on full CV1-train\n",
        "            best_pipe = build_pipeline(imb_mode, clone(est))\n",
        "            if best_params:\n",
        "                best_pipe.set_params(**best_params)\n",
        "            best_pipe.fit(X_tr, y_tr)\n",
        "\n",
        "            # Evaluate on CV1-test\n",
        "            if hasattr(best_pipe, \"predict_proba\"):\n",
        "                y_prob = best_pipe.predict_proba(X_te)[:,1]\n",
        "            else:\n",
        "                y_prob = best_pipe.decision_function(X_te)\n",
        "            metrics = compute_metrics(y_te, y_prob, thr=0.5)\n",
        "            elapsed = round(time.time()-t0, 2)\n",
        "\n",
        "            rows.append({\n",
        "                \"Model\": model_name, \"Imbalance\": imb_mode, \"OuterFold\": ofold,\n",
        "                \"BestParams\": json.dumps(best_params), \"InnerROC_AUC\": inner_auc,\n",
        "                \"Search+RefitTimeSec\": elapsed, **metrics\n",
        "            })\n",
        "\n",
        "    fold_df = pd.DataFrame(rows)\n",
        "    summary = (fold_df.groupby([\"Model\",\"Imbalance\"], as_index=False)\n",
        "               .agg(N=(\"OuterFold\",\"count\"),\n",
        "                    Sensitivity_mean=(\"Sensitivity\",\"mean\"), Sensitivity_sd=(\"Sensitivity\",\"std\"),\n",
        "                    Specificity_mean=(\"Specificity\",\"mean\"), Specificity_sd=(\"Specificity\",\"std\"),\n",
        "                    BAC_mean=(\"BAC\",\"mean\"), BAC_sd=(\"BAC\",\"std\"),\n",
        "                    ROC_AUC_mean=(\"ROC_AUC\",\"mean\"), ROC_AUC_sd=(\"ROC_AUC\",\"std\"),\n",
        "                    PR_AUC_mean=(\"PR_AUC\",\"mean\"), PR_AUC_sd=(\"PR_AUC\",\"std\"),\n",
        "                    TimeSec_sum=(\"Search+RefitTimeSec\",\"sum\")))\n",
        "    return fold_df, summary\n",
        "\n",
        "# -------------------------\n",
        "# Permutation testing (fully nested)\n",
        "# -------------------------\n",
        "def run_permutation_tests(X: pd.DataFrame, y: np.ndarray, observed_summary: pd.DataFrame,\n",
        "                          n_permutations: int, random_state: int = CFG.RANDOM_STATE) -> pd.DataFrame:\n",
        "    rng = check_random_state(random_state)\n",
        "    # We’ll compute null distribution for BAC_mean per (Model,Imbalance)\n",
        "    key_cols = [\"Model\",\"Imbalance\"]\n",
        "    obs = observed_summary[key_cols + [\"BAC_mean\"]].copy().rename(columns={\"BAC_mean\":\"BAC_obs\"})\n",
        "\n",
        "    perm_rows = []\n",
        "    for p in range(1, n_permutations+1):\n",
        "        y_perm = y.copy()\n",
        "        rng.shuffle(y_perm)\n",
        "        fold_df_p, summary_p = outer_cv(X, y_perm)  # full nesting on permuted labels\n",
        "        tmp = summary_p[key_cols + [\"BAC_mean\"]].copy().rename(columns={\"BAC_mean\":\"BAC_perm\"})\n",
        "        tmp[\"perm_idx\"] = p\n",
        "        perm_rows.append(tmp)\n",
        "        print(f\"[perm {p}/{n_permutations}] done\")\n",
        "\n",
        "    perm_df = pd.concat(perm_rows, ignore_index=True)\n",
        "    # p-value = (1 + #perm >= obs) / (1 + n_perm)\n",
        "    merged = perm_df.merge(obs, on=key_cols, how=\"right\")\n",
        "    pvals = (merged\n",
        "             .groupby(key_cols, as_index=False)\n",
        "             .apply(lambda g: pd.Series({\n",
        "                 \"p_value\": (1 + (g[\"BAC_perm\"] >= g[\"BAC_obs\"].iloc[0]).sum()) / (1 + n_permutations)\n",
        "             }))\n",
        "             .reset_index(drop=True))\n",
        "    return pvals\n",
        "\n",
        "# -------------------------\n",
        "# Final training (optional): refit best on full data\n",
        "# -------------------------\n",
        "def train_final_model(X: pd.DataFrame, y: np.ndarray, top_model: Tuple[str,str]) -> ImbPipeline:\n",
        "    \"\"\"top_model = (Model, Imbalance) from observed summary ranking.\"\"\"\n",
        "    # pick estimator & grid, but we won't do search here—use best params from outer CV if desired\n",
        "    specs_map = { (m,imb):(est,grid) for (m,est,grid,imb) in make_model_specs() }\n",
        "    assert top_model in specs_map, f\"Unknown model: {top_model}\"\n",
        "    est, grid = specs_map[top_model]\n",
        "    # Simple approach: one more inner search on all data (quick) then refit full\n",
        "    best_params, _ = inner_cv(X, y, top_model[0], est, grid, top_model[1], random_state=CFG.RANDOM_STATE+777)\n",
        "    pipe = build_pipeline(top_model[1], clone(est))\n",
        "    if best_params:\n",
        "        pipe.set_params(**best_params)\n",
        "    pipe.fit(X, y)\n",
        "    return pipe\n",
        "\n",
        "# =========================\n",
        "# RUN (Oct-20 brief settings)\n",
        "# =========================\n",
        "fold_df, summary_df = outer_cv(X_full, y_full)\n",
        "CFG.RESULTS_DIR = \"/content/drive/MyDrive/tendinopathy/results_nm_full\"\n",
        "import os; os.makedirs(CFG.RESULTS_DIR, exist_ok=True)\n",
        "fold_path = os.path.join(CFG.RESULTS_DIR, \"fold_metrics.csv\")\n",
        "summ_path = os.path.join(CFG.RESULTS_DIR, \"summary_by_model.csv\")\n",
        "fold_df.to_csv(fold_path, index=False)\n",
        "summary_df.to_csv(summ_path, index=False)\n",
        "print(f\"[DONE] Saved fold metrics -> {fold_path}\")\n",
        "print(f\"[DONE] Saved summary     -> {summ_path}\")\n",
        "\n",
        "# point outputs to Drive\n",
        "\n",
        "# (Optional for brief) Permutation tests; consider lowering for quick check\n",
        "pval_df = run_permutation_tests(X_full, y_full, summary_df, n_permutations=CFG.N_PERMUTATIONS)\n",
        "pval_path = os.path.join(CFG.RESULTS_DIR, \"permutation_pvalues.csv\")\n",
        "pval_df.to_csv(pval_path, index=False); print(f\"[DONE] Saved permutation p-values -> {pval_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0TDs2LW-2ZZ",
        "outputId": "11747aea-2167-4f53-a441-4010654187fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive if you want outputs persisted\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# 2) Point EDA output to the same results dir you’ve been using\n",
        "RESULTS_DIR = \"/content/drive/MyDrive/tendinopathy/results_nm_full\"  # same as CFG.RESULTS_DIR\n",
        "import os; os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# 3) Import and run\n",
        "from eda_quality_report import run_eda\n",
        "# assuming your loaded DataFrame is `df` and target column is `target_col`\n",
        "run_eda(df, target_col=target_col, results_dir=RESULTS_DIR)"
      ],
      "metadata": {
        "id": "hxUR9tURYq3e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}